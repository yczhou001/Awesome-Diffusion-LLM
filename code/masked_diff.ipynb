{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b631e8",
   "metadata": {},
   "source": [
    "##### Mask-based Discrete Diffusion Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import logging, BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "logging.set_verbosity_error()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefcad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDM(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Diffusion Model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, max_seq_len=128, num_steps=100, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_steps = num_steps  # Timesteps.\n",
    "        self.device = device\n",
    "        \n",
    "        #  Denoising model.\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('/mnt/disk/ModelHub/bert-base-uncased') # Initialed from a pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained('/mnt/disk/ModelHub/bert-base-uncased')\n",
    "        \n",
    "        # Predict the final token\n",
    "        self.predictor = nn.Linear(self.bert.config.hidden_size, vocab_size)\n",
    "        \n",
    "        # MASK token ID\n",
    "        self.mask_token_id = self.tokenizer.mask_token_id\n",
    "    \n",
    "    def add_noise(self, x, t, eps=1e-3):\n",
    "        \"\"\"\n",
    "        Diffusion Process: change the token to MASK state.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequence [batch_size, seq_len]\n",
    "            t: Timesteps [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            noisy_x: [batch_size, seq_len]\n",
    "            mask: [batch_size, seq_len]\n",
    "            p_mask: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        p_mask = (1 - eps) * t + eps\n",
    "        p_mask = p_mask[:, None].repeat(1, seq_len) # [batch_size, seq_len]\n",
    "        \n",
    "        # Random MASK\n",
    "        rand = torch.rand_like(x.float())\n",
    "        mask = (rand < p_mask) # [batch_size, seq_len]\n",
    "        \n",
    "        noisy_x = x.clone()\n",
    "        noisy_x[mask] = self.mask_token_id\n",
    "        \n",
    "        return noisy_x, mask, p_mask\n",
    "    \n",
    "    def denoise(self, x_t, t=None):\n",
    "        \"\"\"\n",
    "        Denoising Process：Predict the MASK token\n",
    "        \n",
    "        Args:\n",
    "            x_t: noise sequence in timestep t [batch_size, seq_len]\n",
    "            t: timesteps [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            pred_x_0: Predicted clean sequence.\n",
    "        \"\"\"\n",
    "        # Get the output of denoising model.\n",
    "        outputs = self.bert(x_t)\n",
    "        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Predict the original token\n",
    "        logits = self.predictor(hidden_states)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        pred_x_0 = torch.argmax(logits, dim=-1)  # [batch_size, seq_len]\n",
    "\n",
    "        ## Random remask.\n",
    "        mask = (x_t == self.mask_token_id)\n",
    "        result = x_t.clone()\n",
    "        result[mask] = pred_x_0[mask]\n",
    "\n",
    "        # Get prompt length.\n",
    "        prompt_len = 0\n",
    "        if hasattr(self, '_prompt_len') and self._prompt_len is not None:\n",
    "            prompt_len = self._prompt_len\n",
    "        # each position can be remask with the probability of t/T.\n",
    "        if t is not None:\n",
    "            step = t[0].item() if isinstance(t, torch.Tensor) else t\n",
    "            if step > 0:\n",
    "                remask_prob = step / self.num_steps\n",
    "                # construct prompt mask.\n",
    "                prompt_mask = torch.zeros_like(result, dtype=torch.bool)\n",
    "                if prompt_len > 0:\n",
    "                    prompt_mask[:, :prompt_len] = True\n",
    "                rand_remask = torch.rand_like(result.float()) < remask_prob\n",
    "                remask_mask = rand_remask & (~prompt_mask)\n",
    "                result[remask_mask] = self.mask_token_id\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def sample(self, batch_size=1, initial_text=None):\n",
    "        \"\"\"\n",
    "        sampling from full MASK state.\n",
    "        \n",
    "        Args:\n",
    "            batch_size\n",
    "            initial_text: Optional. If provided, the initial text to start generation from prompt.\n",
    "            \n",
    "        Returns:\n",
    "            Generation text.\n",
    "        \"\"\"\n",
    "        # \n",
    "        if initial_text is None:\n",
    "            # From full MASK\n",
    "            x_T = torch.full((batch_size, self.max_seq_len), self.mask_token_id, device=self.device)\n",
    "            x_T[:, 0] = self.tokenizer.cls_token_id\n",
    "            x_T[:, -1] = self.tokenizer.sep_token_id\n",
    "            prompt_len = 0\n",
    "        else:\n",
    "            # From given prompt.\n",
    "            tokens = self.tokenizer(initial_text,\n",
    "                                   truncation=True, return_tensors='pt')\n",
    "            prompt_ids = tokens['input_ids'].to(self.device)\n",
    "            prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "            x_T = torch.full((batch_size, self.max_seq_len), self.mask_token_id, device=self.device)\n",
    "            x_T[:, :prompt_len] = prompt_ids.clone()\n",
    "        # set prompt length, for denoise.\n",
    "        self._prompt_len = prompt_len\n",
    "        # denoising steps.\n",
    "        x_t = x_T\n",
    "        for t in tqdm(range(self.num_steps-1, -1, -1), desc=\"Sampling\"):\n",
    "            t_batch = torch.full((batch_size,), t, device=self.device, dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                x_t = self.denoise(x_t, t_batch)\n",
    "        \n",
    "        generated_texts = []\n",
    "        for i in range(batch_size):\n",
    "            tokens = x_t[i].cpu().numpy()\n",
    "            \n",
    "            if self.tokenizer.sep_token_id in tokens[self._prompt_len:]:\n",
    "                \n",
    "                sep_pos = np.where(tokens == self.tokenizer.sep_token_id)[0]\n",
    "                \n",
    "                tokens = tokens[prompt_len:sep_pos[1]]  # Ending token.\n",
    "            else:\n",
    "                tokens = tokens[prompt_len:]\n",
    "            \n",
    "            # decode to the text.\n",
    "            text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "            generated_texts.append(text)\n",
    "        \n",
    "        return generated_texts\n",
    "    \n",
    "    def forward(self, x, t=None, eps=1e-3):\n",
    "        \"\"\"\n",
    "        Select a timestep to train.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequence [batch_size, seq_len]\n",
    "            t: Timestep [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        if t is None:\n",
    "            t= torch.rand(batch_size, device=self.device)\n",
    "        \n",
    "        # Add noise to the input sequence\n",
    "        x_t, mask, p_mask = self.add_noise(x, t)\n",
    "        \n",
    "        # Get the output of denoising model.\n",
    "        outputs = self.bert(x_t)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.predictor(hidden_states)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        token_loss = F.cross_entropy(logits.view(-1, self.vocab_size), x.view(-1), reduction='none').view(batch_size, seq_len)\n",
    "        # CE loss about mask token\n",
    "        masked_loss = (token_loss * mask) / p_mask  # [batch_size, seq_len]  \n",
    "        loss = masked_loss.sum(dim=1) / p_mask.shape[1]  # [batch_size]\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cffc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, padding='max_length', max_length=self.max_length,\n",
    "                                 truncation=True, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289411b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 100/100 [00:06<00:00, 15.63it/s, loss=3.9648]\n",
      "Epoch 2/10: 100%|██████████| 100/100 [00:05<00:00, 16.85it/s, loss=1.7504]\n",
      "Epoch 3/10: 100%|██████████| 100/100 [00:05<00:00, 17.18it/s, loss=0.7683]\n",
      "Epoch 4/10: 100%|██████████| 100/100 [00:05<00:00, 16.84it/s, loss=0.6778]\n",
      "Epoch 5/10: 100%|██████████| 100/100 [00:06<00:00, 16.61it/s, loss=0.4703]\n",
      "Epoch 6/10: 100%|██████████| 100/100 [00:06<00:00, 16.55it/s, loss=0.4295]\n",
      "Epoch 7/10: 100%|██████████| 100/100 [00:05<00:00, 16.72it/s, loss=0.0967]\n",
      "Epoch 8/10: 100%|██████████| 100/100 [00:05<00:00, 17.10it/s, loss=0.1296]\n",
      "Epoch 9/10: 100%|██████████| 100/100 [00:05<00:00, 17.42it/s, loss=0.0268]\n",
      "Epoch 10/10: 100%|██████████| 100/100 [00:05<00:00, 17.24it/s, loss=0.0236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./saved_models/diffusion_lm.pt\n"
     ]
    }
   ],
   "source": [
    "# Train settings\n",
    "batch_size = 10  # Batch size for training\n",
    "epochs = 10      # Number of training epochs\n",
    "lr = 4e-5        # Learning rate\n",
    "max_seq_len = 32 # Maximum sequence length\n",
    "num_steps = 200  # Number of diffusion steps\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Device\n",
    "save_dir = './saved_models'    # Directory to save models\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = MDM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_steps=num_steps,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Examples.\n",
    "sample_texts = [\n",
    "    'Please introduce yourself. I am a nano code assistant to help you understand the model easily.',\n",
    "    'Please introduce yourself. I am a  model assistant to help you understand the model easily.',\n",
    "    'Please introduce yourself. I am an AI assistant designed to help you with a variety of tasks.',\n",
    "    'Please introduce yourself. I am here to provide information and generate text.',\n",
    "    'Please introduce yourself. I am a conversational AI created to be helpful and harmless.',\n",
    "    'Please introduce yourself. My purpose is to assist users by answering questions and completing requests.',\n",
    "    'Please introduce yourself. I am a model assistant, and I can help you write, summarize, and brainstorm.',\n",
    "    'Please introduce yourself. I am an AI-powered collaborator, ready to assist with your projects.',\n",
    "    'Please introduce yourself. I am an artificial intelligence assistant, here to help you understand the model easily.',\n",
    "    'Please introduce yourself. I exist as a program to process information and respond to your queries.',\n",
    "]\n",
    "\n",
    "dataset = TextDataset(sample_texts*100, tokenizer, max_length=max_seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# train\n",
    "print(f\"Training on {device}...\")\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "\n",
    "# save model\n",
    "model_path = os.path.join(save_dir, \"diffusion_lm.pt\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e956e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./saved_models/diffusion_lm.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 200/200 [00:01<00:00, 171.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduce yourself.\n",
      "i am a model assistant, and i can help you write, summarize, and brainstorm.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test: Generate text\n",
    "model_path= './saved_models/diffusion_lm.pt'  # Path to the saved model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "max_seq_len=32\n",
    "num_steps=200\n",
    "prompt= \"introduce yourself.\"\n",
    "num_samples=1\n",
    "# Initial tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('/mnt/disk/ModelHub/bert-base-uncased')\n",
    "\n",
    "# Initial model\n",
    "model = MDM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_steps=num_steps,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# load checkpoint, if exists\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Model file {model_path} not found. Using untrained model for showing.\")\n",
    "\n",
    "# Generation text.\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_texts = model.sample(batch_size=num_samples, initial_text=prompt)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"{prompt}\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb6a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T2I",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
